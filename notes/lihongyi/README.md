## 李宏毅机器学习任务

**这里是Datawhale第7期组队学习之李宏毅机器学习的相关笔记，非常感谢Datawhale团队。**

#### **参考内容**

- **[李宏毅机器学习课程](https://www.bilibili.com/video/av35932863?from=search&seid=2134843831238226258) **
- [Datawhale整理开源笔记《李宏毅机器学习》](<https://github.com/datawhalechina/Leeml-Book>)
- **《白话大数据与机器学习》** 
- **周志华《机器学习》** 
- **《统计学习方法》** 
- **《机器学习实战》** 
- **吴恩达机器学习教程**

### 第1-3天：线性回归任务一

**学习视频内容**

- 观看李宏毅课程内容：P1、P2。

- 视频地址：[**点我**](<https://www.bilibili.com/video/av35932863?from=search&seid=2134843831238226258>)

**学习打卡任务内容**

- 了解什么是Machine learning

- 学习中心极限定理，学习正态分布，学习最大似然估计

- 推导回归Loss function

- 学习损失函数与凸函数之间的关系

- 了解全局最优和局部最优

- 学习导数，泰勒展开

- 推导梯度下降公式

- 写出梯度下降的代码

- 学习L2-Norm，L1-Norm，L0-Norm

- 推导正则化公式

- 说明为什么用L1-Norm代替L0-Norm

- 学习为什么只对w/Θ做限制，不对b做限制

**我的打卡**

[线性回归任务一](<https://github.com/CrazyXiao/machine-learning/blob/master/notes/lihongyi/day1-3.md>)

### 第4-7天：线性回归任务二

**学习视频内容：**

- **观看**李宏毅课程内容：P4、P5、P6、P7。

- 视频地址：[**点我**](<https://www.bilibili.com/video/av35932863?from=search&seid=8120828691691969718>)

**学习打卡内容：**

- 理解偏差和方差

- 学习误差为什么是偏差和方差而产生的，并且推导数学公式

- 过拟合，欠拟合，分别对应bias和variance什么情况

- 学习鞍点，复习上次任务学习的全局最优和局部最优

- 解决办法有哪些

- 梯度下降

- 学习Mini-Batch与SGD

- 学习Batch与Mini-Batch，SGD梯度下降的区别

- 如何根据样本大小选择哪个梯度下降(批量梯度下降，Mini-Batch）

- 写出SGD和Mini-Batch的代码

- 学习交叉验证

- 学习归一化 

- 学习回归模型评价指标

 **更多的对梯度下降优化将在《李宏毅深度学习》中会有学习任务介绍(指数加权平均，动量梯度下降等)**

**我的打卡**

[线性回归任务一](<https://github.com/CrazyXiao/machine-learning/blob/master/notes/lihongyi/day4-7.md>)

