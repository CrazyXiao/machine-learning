## 李宏毅机器学习Day11-13：机器学习任务四

### 贝叶斯定理

**贝叶斯定理**是关于随机事件 A 和 B 的条件概率：

![img](E:/code/git/machine-learning/notes/lihongyi/images/4.png)

其中P(A|B)是在 B 发生的情况下 A 发生的可能性。

在贝叶斯定理中，每个名词都有约定俗成的名称：

1. `P(A)是 A 的先验概率，之所以称为“先验”是因为它不考虑任何 B 方面的因素。`
2. `P(A|B)是已知 B 发生后 A 的条件概率，也由于得自 B 的取值而被称作 A 的后验概率。`
3. `P(B|A)是已知 A 发生后 B 的条件概率，也由于得自 A 的取值而被称作 B 的后验概率。`
4. `P(B)是 B 的先验概率，也作标淮化常量（normalizing constant）。`

联合概率：表示两个事件共同发生（数学概念上的交集）的概率。A 与 B 的联合概率表示为$P(B∩A)$。

根据条件概率的定义，在事件 B 发生的条件下事件 A 发生的概率为：

$P(A|B)=\frac{P(A\cap B)}{P(B)}​$

同样地，在事件 A 发生的条件下事件 B 发生的概率为：

$P(B|A)=\frac{P(B\cap A)}{P(A)}​$

结合这两个方程式，我们可以得到：

$P(A|B)P(B)  = P(B|A)P(A)  $

上式两边同除以 P(A)，若P(A)是非零的，我们可以得到**贝叶斯定理**:

![img](E:/code/git/machine-learning/notes/lihongyi/images/4.png)

该定理提供了一种计算的逆条件概率的方法，用已知的数据，来求取无法预知数据发生的概率。

这里$P(B)​$ 可以根据**全概率公式**求出。

#### 举例

假设两个盒子，各装了5个球，还得知随机抽一个球，抽到的是盒子1的球的概率是 2/3，是盒子2的球的概率是1/3。从盒子中蓝色球和绿色球的分配可以得到：

- 在盒子1中随机抽一个球，是蓝色的概率为 4/5，绿的的概率为 1/5。
- 在盒子2中随机抽一个球，是蓝色的概率为 2/5，绿的的概率为 3/5。

那么问题来了，随机从两个盒子中抽一个球，抽到的是盒子1中蓝色球的概率是多少？

由题目，已知：

$P(Blue∣B1) = 4/5$

$P(B1) = 2/3$

$P(Blue) = P(Blue∣B1) P(B1) + P(Blue∣B1) P(B2) = 4/5*2/3+2/5*1/3$ = 2/3

基于上面公式，得：

$P(B1∣Blue) = \frac{P(Blue∣B1) P(B1)}{P(Blue)} = 4/5 ​$ 

### 贝叶斯决策论

贝叶斯决策论是概率框架下实施决策的基本方法。它假设决策问题可以用概率的形式来描述，并且假设所有有关的概率结构均已知。

假设$λ_{ij}$为真实标记为$c_j$的样本误分类为$c_i$所产生的损失，可以定义将样本$x$分类$c_i$的条件风险（即期望损失）为

$$R(c_i|x) = \sum_\limits{j=1}^{N} \lambda_{ij}P(c_j|x)​$$

为最小化总体风险，只需要在每个样本上选择那个能使得条件风险$R(c|x)​$最小的类别标记，即

$h(x)=argminR(c|x)​$

这就是贝叶斯最优分类器，此时的总体风险$R(h)$称为贝叶斯风险。

具体来说，若目标是最小化分类错误率，则$λ_{ij}​$ 当i != j时为1，i==j时为0， 此时条件风险为：

$R(c|x) = 1-P(c|x)$

贝叶斯最优分类器为：

$h(x)=argmaxP(c|x)​$

不难看出，要得到最小化决策风险，首先要获得后验概率$P(c|x)$。然而，这在现实任务中通常难以获得。

从这个角度看，机器学习所要实现得就是基于有限得训练样本集尽可能准备地估计出后验概率$P(c|x)​$。

有两种策略，一种是给定$x$，直接建模来预测$c$，这样得到的是**判别式模型**，一种是基于联合概率分布$P(c, x)$

建模，再获得$P(c|x)​$，这样得到的是**生成式模型**。此节我们将要介绍的**贝叶斯分类器**即基于生成式模型。

根据贝叶斯定理，

$P(c∣x) = \frac{P(x∣c) P(c)}{P(x)}​$   

求解$P(c|x)​$的问题转化为求解$P(x∣c)​$, $P(c)​$,$P(x)​$,其中$P(x)​$与类标记无关，$P(c)​$是类先验概率，表示样本空间中各类样本所占的比例,$P(x∣c)​$是类条件概率，可以使用**极大似然法**来估计。

### 朴素贝叶斯

朴素贝叶斯是基于贝叶斯定理与属性条件独立假设的分类方法，对于给定的训练数据集，首先基于属性条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对给定的输入实例x，利用贝叶斯定理求出后验概率最大的输出y。

基于该假定，则有：

$P(c∣x) = \frac{P(x∣c) P(c)}{P(x)} = \frac{P(c)}{P(x)} ∏_\limits{i=0}^{d}P(x_i|c)​$  

其中$d$为属性数目，$x_i$为$x$在第i个属性上的值。

朴素贝叶斯分类器公式：

$h(x)=argmaxP(c|x) =  argmax\frac{P(c)}{P(x)} ∏_\limits{i=0}^{d}P(x_i|c)​$

其中对所有类别来说，$P(x)$相同。

求$P(x_i|c)​$ 时，对于连续属性可考虑概率密度函数。

### 先验概率和后验概率

先验概率是指根据以往经验和分析得到的概率，而不依赖于其他方面的因素。

后验概率是指事件发生的可能原因的概率。

### LR和linear regression区别

这里的LR意值逻辑回归。

逻辑回归和线性回归都属于广义线性回归。

区别如下：

1. 线性回归的输出是连续值，逻辑回归的输出区间为[0,1]，可简单理解为事件发生的概率，基于该概率做分类。
2. 逻辑回归是线性回归被sigmoid函数映射后的结果。

### 推导sigmoid function公式

根据贝叶斯定理及高斯分布，我们可以推导**s函数**。

过程如下：

