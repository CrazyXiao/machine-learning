## 李宏毅机器学习Day30-33:CART算法

### CART(分类回归树)

属于决策树的一种，由特征选择、树的生成以及剪枝组成，既可以用于分类也可以用于回归。CART算法采用的是一种二分递归分割的技术，将当前样本分成两个子样本集，使得生成的非叶子节点都有两个分支。因此CART实际上是一棵**二叉树**。

CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。

**对分类树而言，CART用Gini系数最小化准则来进行特征选择，生成二叉树。当CART作为回归树的时候，使用样本的最小方差作为分裂节点的依据。**

### 基尼(Gini)指数

定义如下：
$$
Gini{index}= \sum^V_\limits{v=1} \frac{|D^v|}{D} Gini(D^v)，而
Gini(D)= 1- \sum^y_{k=1} p_k^2
$$
$Gini(D)$反映了从数据集$D$中随机抽取两个样本，其类别标记不一致的概率。因此$Gini(D)$越小，则数据集$D$的纯度越高。

### 生成分类树

**输入：**训练数据集$D​$，停止计算的条件：结点中的样本个数小于预定阈值，或样本集的Gini系数小于预定阈值（样本基本属于同一类），或者没有更多特征。 
**输出：**CART决策树。

根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：

1. 设结点的训练数据集为$D$，计算现有特征对该数据集的Gini系数。此时，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或 “否”将$D$分割成$D1$和$D2$两部分，计算$A=a$时的Gini系数。
2. 在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择Gini系数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。
3. 对两个子结点递归地调用步骤1~2，直至满足停止条件。
4. 生成CART决策树。

计算过程类似ID3，只是选择最优特征的依据不同而已。

### 生成回归树

**输入：**训练数据集$D $
**输出：**回归树$f(x)$

在训练数据集所在的输入空间中，递归得将每一个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：

1. 选择最优切分变量j和切分点s，求解 
   $$
   min_{j,s}[min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]
   $$
   

   遍历变量j，对固定的切分变量j扫描切分点s，选择使上式达到误差最小的变量(j,s)，其中$R1$和$R2​$表示的是划分之后的空间。

2. 用选定的(j,s)划分区域并决定响应的输出值。
   $$
   R_1(j,s)=\{x^{(j)}\}\leq s,\quad R_2(j,s)=\{x|x^{(j)}>s\}
   $$
   
   $$
   c_m=\frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i,\quad x\in R_m,m=1,2
   $$

3. 继续对两个子区域调用步骤1~2,直到满足停止条件。

4. 将输入空间划分为M个区域$R_1,R_2,R_3....R_M$，生成决策树：
   $$
   f(x)=\sum_{m=1}^Mc_mI(x\in R_m)
   $$



------

##### 参考

[CART算法的原理以及实现](<https://blog.csdn.net/gzj_1101/article/details/78355234>)

