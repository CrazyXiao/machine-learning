## 李宏毅机器学习Day14-16：逻辑回归

### 推导LR损失函数

假设我们有一组训练集，类别为c1，c2，数据基于后验概率$f_{w,b}(x) = P_{w,b}(c_1|x) = g(z)​$产生。

其中$g\left( z \right)=\frac{1}{1+{{e}^{-z}}}$，$z=\sum_\limits{i=0}^{d}w_ix_i+b$，$d$表示属性数目。

![img](../../notes/lihongyi/images/5.png)

基于**最大似然法**，有：

$L_{(w,b)}=f_{w,b}(x^1)f_{w,b}(x^2)(1−f_{w,b}(x^3))⋯f_{w,b}(x^N)$

对于使得 $L(w,b)​$最大的$w​$和 $b​$，记做$w^∗​$和 $b^∗​$，即：

$w^∗, b^∗=argmaxL(w,b) ​$

对其取负自然对数后，转化为：

$w^∗, b^∗=argmaxL(w,b) = argmin(-lnL(w,b))​$，

其中，有：

$-lnL(w,b)=-lnf_{w,b}(x^1)-lnf_{w,b}(x^2)-ln(1−f_{w,b}(x^3))⋯-lnf_{w,b}(x^N)$

$=\sum_\limits{i=0}^{m}-[y^ilnf(x^i)+(1-y^i)ln(1-f(x^i))]$

$y^i$代表C1或者C2所对应的数值，这里$f_{w,b}(x)$对应的C1，所以表示C1时，$y^i$=1，表示C2时，$y^i​$=0。

### 学习LR梯度下降

