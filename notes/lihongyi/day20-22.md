## 李宏毅机器学习Day20-22:决策树

### 信息论

#### 信息熵（香浓熵）

信息熵是度量样本纯度的一种常用指标，代表随机变量的复杂度，其越小则纯度越高。定义如下：
$$
Ent(D)=−\sum_\limits{k=1}^yp_klog_2p_k
$$

其中满足：

$0⩽H(p)⩽log_2n$, 这里 $H(p)$ 等同于$Ent(D)$。

证明如下：

![img](../../notes/lihongyi/images/2.jpg)



#### 相对熵

相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度来衡量这两个分布的差异。

在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1] 。直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，Q等价于P。

公式如下：

$D_{KL}(p||q)=\sum_\limits{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)}) $

#### 交叉熵

$$
\begin{eqnarray}
D_{KL}(p||q) &=& \sum_{i=1}^np(x_i)log(p(x_i))-\sum_{i=1}^np(x_i)log(q(x_i))\\
&=& -H(p(x))+[-\sum_{i=1}^np(x_i)log(q(x_i))]
\end{eqnarray}
$$

等式的前一部分恰巧就是p的熵，等式的后一部分，就是交叉熵。

在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即$D_{KL}(y||\hat{y})$，由于KL散度中的前一部分不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用用交叉熵做loss，评估模型。

#### 信息增益

条件熵代表在某一个条件下，随机变量的复杂度（不确定度）。

信息增益=信息熵-条件熵，代表了在一个条件下，信息复杂度（不确定性）减少的程度。

#### CART算法

使用**基尼指数**划分属性，定义如下：
$$
Gini{index}= \sum^V_\limits{v=1} \frac{|D^v|}{D} Gini(D^v)，而
Gini(D)= 1- \sum^y_{k=1} p_k^2
$$
$Gini(D)$反映了从数据集$D$中随机抽取两个样本，其类别标记不一致的概率。因此$Gini(D)$越小，则数据集$D​$的纯度越高。

### 剪枝

- 预剪枝：在构造决策树的同时进行剪枝，对每个节点进行划分前的估计，如果不能带来决策树泛化性能的提升则停止划分并将当前节点标记为叶节点。预剪枝有带来欠拟合的风险。
- 后剪枝：决策树构造完成后进行剪枝，自底向上对非叶节点考察，如果该节点的子树替换为子树的叶节点可以提升泛化性能，则替换该子树为其叶节点。后剪纸的欠拟合风险很小，泛化性能通常优于预剪枝，但计算量大，训练时间较长。

### 计算香浓熵

```python
import numpy as np
import pandas as pd
from collections import Counter

def calcShannonEnt(data):
    """ 计算信息熵
    """
    # 获取最后一列的数据
    labels = data[data.columns.values[-1]]
    # 统计所有类别对应出现的次数
    labelCounts = Counter(labels)
    # 数据已准备好，计算熵
    shannonEnt = 0.0
    dataLen = len(data)
    for key in labelCounts:
        pro = labelCounts[key] / dataLen
        shannonEnt -= pro * np.log2(pro)
    return shannonEnt

data = pd.read_csv("watermelon_3a.csv")
res = calcShannonEnt(data)
print("香浓熵为:", res)
```

输出结果如下：

```
香浓熵为: 0.9975025463691153
```

------

相关资料：

[决策树算法十问](<https://blog.csdn.net/Datawhale/article/details/90605363>)

[Datawhale决策树笔记](<https://datawhalechina.github.io/Leeml-Book/#/AdditionalReferences/Entropy>)

