{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二版：通过catboost做个简单的二分类来判断是否作弊，提交后得分为：94.33074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "class Processing():\n",
    "    #黑名单获取\n",
    "    def _get_blacklist(self,train):\n",
    "        cheat = train[train['label']==1] \n",
    "        noCheat = train[train['label']==0] \n",
    "        blacklist_dic = {}\n",
    "        for f in ['adidmd5','imeimd5']:\n",
    "            w_s = []\n",
    "            s = set(cheat[f])\n",
    "            blacklist_dic[f] = s\n",
    "        return blacklist_dic\n",
    "    #特征工程 \n",
    "    def _feature_eng(self,train,test):\n",
    "        features = ['pkgname', 'ver', 'adunitshowid', 'mediashowid', 'apptype', 'adidmd5', 'imeimd5', 'ip','macmd5', 'openudidmd5',\n",
    "            'reqrealip', 'city', 'province', 'idfamd5', 'dvctype', 'model', 'make', 'ntt',\n",
    "            'carrier', 'os', 'osv', 'orientation', 'lan', 'h', 'w', 'ppi']\n",
    "        train_test = pd.concat([train, test], ignore_index=True,sort=True)\n",
    "        train_test['label'] = train_test['label'].fillna(-1).astype(int)\n",
    "        train_test['time'] = pd.to_datetime(train_test['nginxtime'] , unit='ms')\n",
    "        train_test['hour'] = train_test['time'].dt.hour.astype('str')\n",
    "        train_test.fillna('null_value',inplace = True)\n",
    "        features.append('hour')\n",
    "        new_test = train_test[train_test['label'] == -1]\n",
    "        new_train = train_test[train_test['label'] != -1]\n",
    "        return new_train,new_test,features\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class TrainModels():  \n",
    "    #黑名单作弊判断\n",
    "    def _judge_black(self,blacklist_dic,test):\n",
    "        judge_cheat_sid = set()\n",
    "        judge_features = list(blacklist_dic.keys())\n",
    "        judge_df = test[judge_features+['sid']]\n",
    "        judge_df['label'] = [0]*len(judge_df)\n",
    "        for f in judge_features:\n",
    "            s = blacklist_dic[f]\n",
    "            judge_df['label'] = judge_df.apply(lambda x: 1 if (x[f] in s or x['label'] == 1) else 0,axis=1)\n",
    "        return judge_df[['sid','label']]\n",
    "    \n",
    "    #利用catboost做二分类\n",
    "    def _judge_catboost(self,train,test,features):    \n",
    "        model = CatBoostClassifier(iterations=946, depth=8,cat_features=features,learning_rate=0.05, custom_metric='F1',eval_metric='F1',random_seed=2019,\n",
    "                            l2_leaf_reg=5.0,logging_level='Silent')\n",
    "        model.fit(train[features],train['label'])\n",
    "        y_pred = model.predict(test[features]).tolist()\n",
    "        \n",
    "        judge_df = pd.DataFrame()\n",
    "        judge_df['sid'] = test['sid'].tolist()\n",
    "        judge_df['label'] = y_pred\n",
    "        judge_df['label'] = judge_df['label'].apply(lambda x: 1 if x>=0.5 else 0)\n",
    "        return judge_df[['sid','label']]\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    train = pd.read_table('round1_iflyad_anticheat_traindata.txt')\n",
    "    test = pd.read_table('round1_iflyad_anticheat_testdata_feature.txt')    \n",
    "    proce_module = Processing()\n",
    "    model_module = TrainModels()\n",
    "    #黑名单\n",
    "    blacklist_dic = proce_module._get_blacklist(train)\n",
    "    judge_by_blackList = model_module._judge_black(blacklist_dic,test)\n",
    "    judge_by_blackList.to_csv('judge_by_blackList.csv',index=False,encoding='utf-8')\n",
    "    #二分类---使用catboost\n",
    "    new_train,new_test,features= proce_module._feature_eng(train,test)\n",
    "    print('ok')\n",
    "    judge_by_catboost = model_module._judge_catboost(new_train,new_test,features)   \n",
    "    judge_by_catboost.to_csv('judge_by_catboost.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "对大量类别特征进行了Count/Rank编码处理\n",
    "对长尾分布的特征有较好的表达\n",
    "清洗了Model及Make,处理了长尾分布\n",
    "全部大写\n",
    "替换了url标识符\n",
    "使用了Catboost建模\n",
    "适用于Category类型较多的场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MindRank.ai\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as DF\n",
    "import scipy.spatial.distance as dist\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cbt\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import gc\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from six.moves import reduce\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "train = pd.read_table(\"../input/round1_iflyad_anticheat_traindata.txt\")\n",
    "test = pd.read_table(\"../input/round1_iflyad_anticheat_testdata_feature.txt\")\n",
    "all_data = train.append(test).reset_index(drop=True)\n",
    "\n",
    "# 对时间的处理\n",
    "all_data['time'] = pd.to_datetime(all_data['nginxtime']*1e+6) + timedelta(hours=8)\n",
    "all_data['day'] = all_data['time'].dt.dayofyear\n",
    "all_data['hour'] = all_data['time'].dt.hour\n",
    "\n",
    "# Data Clean\n",
    "# 全部变成大写，防止oppo 和 OPPO 的出现\n",
    "all_data['model'].replace('PACM00',\"OPPO R15\",inplace=True)\n",
    "all_data['model'].replace('PBAM00',\"OPPO A5\",inplace=True)\n",
    "all_data['model'].replace('PBEM00',\"OPPO R17\",inplace=True)\n",
    "all_data['model'].replace('PADM00',\"OPPO A3\",inplace=True)\n",
    "all_data['model'].replace('PBBM00',\"OPPO A7\",inplace=True)\n",
    "all_data['model'].replace('PAAM00',\"OPPO R15_1\",inplace=True)\n",
    "all_data['model'].replace('PACT00',\"OPPO R15_2\",inplace=True)\n",
    "all_data['model'].replace('PABT00',\"OPPO A5_1\",inplace=True)\n",
    "all_data['model'].replace('PBCM10',\"OPPO R15x\",inplace=True)\n",
    "\n",
    "for fea in ['model','make','lan']:\n",
    "    all_data[fea] = all_data[fea].astype('str')\n",
    "    all_data[fea] = all_data[fea].map(lambda x:x.upper())\n",
    "\n",
    "    from urllib.parse import unquote\n",
    "\n",
    "    def url_clean(x):\n",
    "        x = unquote(x,'utf-8').replace('%2B',' ').replace('%20',' ').replace('%2F','/').replace('%3F','?').replace('%25','%').replace('%23','#').replace(\".\",' ').replace('??',' ').\\\n",
    "                               replace('%26',' ').replace(\"%3D\",'=').replace('%22','').replace('_',' ').replace('+',' ').replace('-',' ').replace('__',' ').replace('  ',' ').replace(',',' ')\n",
    "        \n",
    "        if (x[0]=='V') & (x[-1]=='A'):\n",
    "            return \"VIVO {}\".format(x)\n",
    "        elif (x[0]=='P') & (x[-1]=='0'):\n",
    "            return \"OPPO {}\".format(x)\n",
    "        elif (len(x)==5) & (x[0]=='O'):\n",
    "            return \"Smartisan {}\".format(x)\n",
    "        elif ('AL00' in x):\n",
    "            return \"HW {}\".format(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    all_data[fea] = all_data[fea].map(url_clean)\n",
    "    \n",
    "all_data['big_model'] = all_data['model'].map(lambda x:x.split(' ')[0])\n",
    "all_data['model_equal_make'] = (all_data['big_model']==all_data['make']).astype(int)\n",
    "\n",
    "# H,W,PPI\n",
    "\n",
    "all_data['size'] = (np.sqrt(all_data['h']**2 + all_data['w'] ** 2) / 2.54) / 1000\n",
    "all_data['ratio'] = all_data['h'] / all_data['w']\n",
    "all_data['px'] = all_data['ppi'] * all_data['size']\n",
    "all_data['mj'] = all_data['h'] * all_data['w']\n",
    "\n",
    "num_col = ['h','w','size','mj','ratio','px']\n",
    "cat_col = [i for i in all_data.select_dtypes(object).columns if i not in ['sid','label']]\n",
    "both_col = []\n",
    "\n",
    "for i in tqdm(cat_col):\n",
    "    lbl = LabelEncoder()\n",
    "    all_data[i+\"_count\"] = all_data.groupby([i])[i].transform('count')\n",
    "    all_data[i+\"_rank\"] = all_data[i+\"_count\"].rank(method='min')\n",
    "    all_data[i] = lbl.fit_transform(all_data[i].astype(str))\n",
    "    both_col.extend([i+\"_count\",i+\"_rank\"])\n",
    "\n",
    "for i in tqdm(['h','w','ppi','ratio']):\n",
    "    all_data['{}_count'.format(i)] = all_data.groupby(['{}'.format(i)])['sid'].transform('count')\n",
    "    all_data['{}_rank'.format(i)] = all_data['{}_count'.format(i)].rank(method='min')\n",
    "\n",
    "feature_name = [i for i in all_data.columns if i not in ['sid','label','time']]\n",
    "cat_list = [i for i in train.columns if i not in ['sid','label','nginxtime']]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "tr_index = ~all_data['label'].isnull()\n",
    "X_train = all_data[tr_index][list(set(feature_name))].reset_index(drop=True)\n",
    "y = all_data[tr_index]['label'].reset_index(drop=True).astype(int)\n",
    "X_test = all_data[~tr_index][list(set(feature_name))].reset_index(drop=True)\n",
    "print(X_train.shape,X_test.shape)\n",
    "random_seed = 2019\n",
    "final_pred = []\n",
    "cv_score = []\n",
    "cv_model = []\n",
    "skf = StratifiedKFold(n_splits=5, random_state=random_seed, shuffle=True)\n",
    "for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n",
    "    print(index)\n",
    "    train_x, test_x, train_y, test_y = X_train[feature_name].iloc[train_index], X_train[feature_name].iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "    cbt_model = cbt.CatBoostClassifier(iterations=3000,learning_rate=0.05,max_depth=11,l2_leaf_reg=1,verbose=10,early_stopping_rounds=400,task_type='GPU',eval_metric='F1',cat_features=cat_list)\n",
    "    cbt_model.fit(train_x[feature_name], train_y,eval_set=(test_x[feature_name],test_y))\n",
    "    cv_model.append(cbt_model)\n",
    "    y_test = cbt_model.predict(X_test[feature_name])\n",
    "    y_val = cbt_model.predict_proba(test_x[feature_name])\n",
    "    print(Counter(np.argmax(y_val,axis=1)))\n",
    "    cv_score.append(f1_score(test_y,np.round(y_val[:,1])))\n",
    "\n",
    "# Catboost比较适合类别较多的场景\n",
    "    \n",
    "# GPU结果五折 \n",
    "# 第一折\n",
    "# bestTest = 0.94051\n",
    "# bestIteration = 1512\n",
    "\n",
    "fi = []\n",
    "for i in cv_model:\n",
    "    tmp = {\n",
    "        'name' : feature_name,\n",
    "        'score' : i.feature_importances_\n",
    "    }\n",
    "    fi.append(pd.DataFrame(tmp))\n",
    "    \n",
    "fi = pd.concat(fi)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "fi.groupby(['name'])['score'].agg('mean').sort_values(ascending=False).head(40).plot.barh()\n",
    "\n",
    "cv_pred = np.zeros((X_train.shape[0],))\n",
    "test_pred = np.zeros((X_test.shape[0],))\n",
    "for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n",
    "    print(index)\n",
    "    train_x, test_x, train_y, test_y = X_train.iloc[train_index], X_train.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "    y_val = cv_model[index].predict_proba(test_x[feature_name])[:,1]\n",
    "    print(y_val.shape)\n",
    "    cv_pred[test_index] = y_val\n",
    "    test_pred += cv_model[index].predict_proba(X_test[feature_name])[:,1] / 5\n",
    "\n",
    "print(\"CV score: \",np.mean(cv_score))\n",
    "\n",
    "submit = test[['sid']]\n",
    "submit['label'] = (test_pred>=0.5).astype(int)\n",
    "print(submit['label'].value_counts())\n",
    "submit.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGB+Catboost+卡方校验 94.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2019/7/25 20:47\n",
    "# @Author  : YYLin\n",
    "# @Email   : 854280599@qq.com\n",
    "# @File    : My_Method_For_LGB.py\n",
    "# 本版本的变化有三个   删除了一些缺失值较多的属性 以及认为不重要的属性\n",
    "# 增加一些混合属性    关于IP地址 app类型和广告位之间的数据统计\n",
    "# 最后显示了数据集中哪些特征是重要特征\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from datetime import timedelta, datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    " \n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    " \n",
    "# 定义训练使用的数据列\n",
    "train_cols = ['ip', 'apptype', 'model', 'os', 'adunitshowid', 'mediashowid', 'nginxtime', 'label', 'sid']\n",
    "test_cols = ['ip', 'apptype', 'model', 'os', 'adunitshowid', 'mediashowid', 'nginxtime', 'sid']\n",
    " \n",
    "# 读取训练集以及测试集 并进行拼接操作\n",
    "df_train = pd.read_csv('data/traindata.txt', sep='\\t', usecols=train_cols)\n",
    "df_test = pd.read_csv('data/testdata.txt', sep='\\t', usecols=test_cols)\n",
    " \n",
    "All_data_for_train = pd.concat([df_train, df_test], ignore_index=True).drop(columns='sid')\n",
    "All_data_for_train['label'] = All_data_for_train['label'].fillna(-1).astype(int)\n",
    "# print('检查一下数据是否读取正确:\\n', All_data_for_train.head(5))\n",
    " \n",
    "# 在训练集中增加 day hour mintue\n",
    "All_data_for_train['datetime'] = pd.to_datetime(All_data_for_train['nginxtime'] / 1000, unit='s') + timedelta(hours=8)\n",
    "All_data_for_train['hour'] = All_data_for_train['datetime'].dt.hour\n",
    "All_data_for_train['day'] = All_data_for_train['datetime'].dt.day - All_data_for_train['datetime'].dt.day.min()\n",
    "All_data_for_train['minute'] = All_data_for_train['datetime'].dt.minute.astype('uint8')\n",
    "All_data_for_train.drop(['nginxtime'], axis=1, inplace=True)\n",
    "# print('检查一下数据中的时间信息是否正确:\\n', All_data_for_train.head(5))\n",
    " \n",
    "# 对model---设备进行处理\n",
    "All_data_for_train['model'].replace('PACM00', \"OPPO R15\", inplace=True)\n",
    "All_data_for_train['model'].replace('PBAM00', \"OPPO A5\", inplace=True)\n",
    "All_data_for_train['model'].replace('PBEM00', \"OPPO R17\", inplace=True)\n",
    "All_data_for_train['model'].replace('PADM00', \"OPPO A3\", inplace=True)\n",
    "All_data_for_train['model'].replace('PBBM00', \"OPPO A7\", inplace=True)\n",
    "All_data_for_train['model'].replace('PAAM00', \"OPPO R15_1\", inplace=True)\n",
    "All_data_for_train['model'].replace('PACT00', \"OPPO R15_2\", inplace=True)\n",
    "All_data_for_train['model'].replace('PABT00', \"OPPO A5_1\", inplace=True)\n",
    "All_data_for_train['model'].replace('PBCM10', \"OPPO R15x\", inplace=True)\n",
    " \n",
    "# 处理属性中出现的大小写问题\n",
    "All_data_for_train['model'] = All_data_for_train['model'].astype('str')\n",
    "All_data_for_train['model'] = All_data_for_train['model'].map(lambda x: x.upper())\n",
    "All_data_for_train['os'] = All_data_for_train['os'].astype('str')\n",
    "All_data_for_train['os'] = All_data_for_train['os'].map(lambda x: x.upper())\n",
    " \n",
    "# 统计属性列中单个属性出现次数 以及对结果进行排序 可以发现统计列的值比较的大\n",
    "# 版本分为两个 一个是使用单独属性 一个是不使用单独属性分别测试\n",
    "print('loading single attributes ...........\\n')\n",
    "cols_for_single_atr = ['ip', 'apptype', 'model', 'os', 'adunitshowid', 'mediashowid']\n",
    "for i in tqdm(cols_for_single_atr):\n",
    "    lbl = LabelEncoder()\n",
    "    All_data_for_train[i + \"_count\"] = All_data_for_train.groupby([i])[i].transform('count')\n",
    "    All_data_for_train[i + \"_rank\"] = All_data_for_train[i + \"_count\"].rank(method='min')\n",
    "    All_data_for_train[i] = lbl.fit_transform(All_data_for_train[i].astype(str))\n",
    "# print('使用groupby之后数据的信息是:\\n', All_data_for_train.head(5))\n",
    " \n",
    "# 开始统计一些复合属性\n",
    "# 第一列统计的是： 通过什么app访问的这个广告\n",
    "# 第二列统计的是： 那个IP地址访问了这个广告\n",
    "print('\\nloading Fusion_attributes........\\n')\n",
    "Fusion_attributes = ['apptype_adunitshowid', 'apptype_adunitshowid_mediashowid', 'apptype_mediashowid', 'apptype_adunitshowid_model_day_hour',\n",
    "                     'apptype_model_day_hour', 'apptype_os_adunitshowid_day_hour',\n",
    " \n",
    "                     'ip_day', 'ip_apptype_model_adunitshowid_day', 'ip_apptype_model_day', 'ip_apptype_model_day_os_hour',\n",
    "                     'ip_apptype_os_adunitshowid', 'ip_os', 'ip_apptype_os_adunitshowid_day']\n",
    " \n",
    "for attribute in tqdm(Fusion_attributes):\n",
    "    name = \"count_\" + attribute\n",
    "    dummy = 'label'\n",
    "    cols = attribute.split(\"_\")\n",
    "    cols_with_dummy = cols.copy()\n",
    "    cols_with_dummy.append(dummy)\n",
    "    gp = All_data_for_train[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: name})\n",
    "    All_data_for_train = All_data_for_train.merge(gp, on=cols, how='left')\n",
    "# print('经过融合属性之后数据中的值是:\\n', All_data_for_train.head(5))\n",
    " \n",
    "# 开始统计一些比值信息\n",
    "print('\\n loading Ratio for model: .........\\n')\n",
    " \n",
    "All_data_for_train[\"machine\"] = 1000*All_data_for_train[\"model\"] + All_data_for_train[\"os\"]\n",
    "Ratio_Attributes = ['ip_machine', 'machine_ip', 'apptype_adunitshowid', 'adunitshowid_apptype', 'apptype_mediashowid',\n",
    "                    'mediashowid_apptype']\n",
    "for attribute in Ratio_Attributes:\n",
    "    name = \"countRatio_\" + attribute\n",
    "    dummy = 'label'\n",
    "    cols = attribute.split(\"_\")\n",
    "    cols_with_dummy = cols.copy()\n",
    "    cols_with_dummy.append(dummy)\n",
    " \n",
    "    # 进行属性比值的融合\n",
    "    gp1 = All_data_for_train[cols_with_dummy].groupby(by=cols)[[dummy]].count().reset_index().rename(index=str, columns={dummy: 'cnt1'})\n",
    "    _df = All_data_for_train.merge(gp1, on=cols, how='left')\n",
    "    gp2 = All_data_for_train[cols].groupby(by=cols[0:len(cols) - 1])[[cols[len(cols) - 1]]].count().reset_index().rename(index=str, columns={cols[len(cols) - 1]: 'cnt2'})\n",
    "    _df['cnt2'] = All_data_for_train.merge(gp2, on=cols[0:len(cols) - 1], how='left')['cnt2']\n",
    " \n",
    "    All_data_for_train[name] = _df['cnt1'] / _df['cnt2']\n",
    "# print('经过属性比值融合之后的属性是:', All_data_for_train.head(5))\n",
    " \n",
    "All_data_for_train = All_data_for_train.drop(columns='datetime')\n",
    "Y_data = All_data_for_train.loc[All_data_for_train['label'] != -1]['label']\n",
    "X_data = All_data_for_train.loc[All_data_for_train['label'] != -1].drop(columns='label')\n",
    "# print('训练集中的数据格式:\\n', X_data.head(5))\n",
    "# print('训练集中标签的格式:\\n', Y_data.head(5))\n",
    " \n",
    "X_test = All_data_for_train.loc[All_data_for_train['label'] == -1].drop(columns='label')\n",
    "# print('测试集中的数据格式:\\n', X_test.head(5))\n",
    " \n",
    "train_x, val_x, train_y, val_y = train_test_split(X_data, Y_data, test_size=0.2)\n",
    " \n",
    " \n",
    "def lgb_f1(labels, preds):\n",
    "    score = f1_score(labels, np.round(preds))\n",
    "    return 'f1', score, True\n",
    " \n",
    " \n",
    "# 增加了学习率 并且增大了运行的次数\n",
    "lgb = LGBMClassifier(random_seed=2019, n_jobs=-1, objective='binary',\n",
    "                     learning_rate=0.1, n_estimators=6500, num_leaves=31, max_depth=-1,\n",
    "                     min_child_samples=50, min_child_weight=9, subsample_freq=1,\n",
    "                     subsample=0.7, colsample_bytree=0.7, reg_alpha=1, reg_lambda=5)\n",
    " \n",
    "lgb.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    eval_set=[(train_x, train_y), (val_x, val_y)],\n",
    "    eval_names=['train', 'val'],\n",
    "    eval_metric=lgb_f1,\n",
    "    early_stopping_rounds=400,\n",
    "    verbose=10,\n",
    ")\n",
    "print('best score', lgb.best_score_)\n",
    " \n",
    "# %%-------------------------------\n",
    "print('predict')\n",
    "lgb.n_estimators = lgb.best_iteration_\n",
    "lgb.fit(X_data, Y_data)\n",
    "test_y = lgb.predict(X_test)\n",
    "df_sub = pd.concat([df_test['sid'], pd.Series(test_y)], axis=1)\n",
    "df_sub.columns = ['sid', 'label']\n",
    "df_sub.to_csv('lgb_submit-{}.csv'.format(datetime.now().strftime('%m%d_%H%M%S')), sep=',', index=False)\n",
    " \n",
    "# 2019 7 25 画图显示模型的重要特征\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " \n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    " \n",
    "features_list = X_data.columns.values\n",
    "feature_importance = lgb.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    " \n",
    "plt.figure(figsize=(5, 7))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), features_list[sorted_idx])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature importances')\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: shaowu\n",
    "注：此次会详细注释代码，往后都省略。\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import tqdm\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "def one_hot_col(col):\n",
    "    '''标签编码'''\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(col)\n",
    "    return lbl\n",
    "def calculate_null(data,key,col):\n",
    "    '''\n",
    "    params:\n",
    "    data -- input data\n",
    "    key -- the key used for statistics\n",
    "    col -- the columns for statistics\n",
    "    return -- the data of DataFrame type, include two columns,\n",
    "              first columns id key,second is number of null\n",
    "    '''\n",
    "    return data.groupby(key,as_index=False)[col].agg({col+'_is_null':'count'})\n",
    "def xgb_model(new_train,y,new_test,lr):\n",
    "    '''定义模型'''\n",
    "    xgb_params = {'booster': 'gbtree',\n",
    "          'eta':lr, 'max_depth': 5, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "          'objective':'binary:logistic',\n",
    "          'eval_metric': 'auc',\n",
    "          'silent': True,\n",
    "          }\n",
    "    #skf=StratifiedKFold(y,n_folds=5,shuffle=True,random_state=2018)\n",
    "    skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "    oof_xgb=np.zeros(new_train.shape[0])\n",
    "    prediction_xgb=np.zeros(new_test.shape[0])\n",
    "    for i,(tr,va) in enumerate(skf.split(new_train,y)):\n",
    "        print('fold:',i+1,'training')\n",
    "        dtrain = xgb.DMatrix(new_train[tr],y[tr])\n",
    "        dvalid = xgb.DMatrix(new_train[va],y[va])\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'valid_data')]\n",
    "        bst = xgb.train(dtrain=dtrain, num_boost_round=30000, evals=watchlist, early_stopping_rounds=200, \\\n",
    "        verbose_eval=50, params=xgb_params)\n",
    "        oof_xgb[va] += bst.predict(xgb.DMatrix(new_train[va]), ntree_limit=bst.best_ntree_limit)\n",
    "        prediction_xgb += bst.predict(xgb.DMatrix(new_test), ntree_limit=bst.best_ntree_limit)\n",
    "    print('the roc_auc_score for train:',roc_auc_score(y,oof_xgb))\n",
    "    prediction_xgb/=5\n",
    "    return oof_xgb,prediction_xgb\n",
    "def lgb_model(new_train,y,new_test):\n",
    "    params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'num_leaves': 1000,\n",
    "    'verbose': -1,\n",
    "    'max_depth': -1,\n",
    "  #  'reg_alpha':2.2,\n",
    "  #  'reg_lambda':1.4,\n",
    "    'seed':42,\n",
    "    }\n",
    "    #skf=StratifiedKFold(y,n_folds=5,shuffle=True,random_state=2018)\n",
    "    skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "    oof_lgb=np.zeros(new_train.shape[0]) ##用于存放训练集概率，由每折验证集所得\n",
    "    prediction_lgb=np.zeros(new_test.shape[0])  ##用于存放测试集概率，k折最后要除以k取平均\n",
    "    feature_importance_df = pd.DataFrame() ##存放特征重要性，此处不考虑\n",
    "    for i,(tr,va) in enumerate(skf.split(new_train,y)):\n",
    "        print('fold:',i+1,'training')\n",
    "        dtrain = lgb.Dataset(new_train[tr],y[tr])\n",
    "        dvalid = lgb.Dataset(new_train[va],y[va],reference=dtrain)\n",
    "        ##训练：\n",
    "        bst = lgb.train(params, dtrain, num_boost_round=30000, valid_sets=dvalid, verbose_eval=400,early_stopping_rounds=200)\n",
    "        ##预测验证集：\n",
    "        oof_lgb[va] += bst.predict(new_train[va], num_iteration=bst.best_iteration)\n",
    "        ##预测测试集：\n",
    "        prediction_lgb += bst.predict(new_test, num_iteration=bst.best_iteration)\n",
    "        '''\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = list(new_train.columns)\n",
    "        fold_importance_df[\"importance\"] = bst.feature_importance(importance_type='split', iteration=bst.best_iteration)\n",
    "        fold_importance_df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        '''\n",
    "    \n",
    "    print('the roc_auc_score for train:',roc_auc_score(y,oof_lgb)) ##线下auc评分\n",
    "    prediction_lgb/=5\n",
    "    return oof_lgb,prediction_lgb,feature_importance_df\n",
    "\n",
    "##读入测试数据：\n",
    "testdata= pd.read_csv(\"round1_iflyad_anticheat_testdata_feature.txt\",sep='\\t')\n",
    "testdata['label']=-1 ##测试集没有标签，可标记为-1\n",
    "\n",
    "testdata['begin_time']=testdata['sid'].apply(lambda x:int(x.split('-')[-1])) ##请求会话时间\n",
    "testdata['nginxtime-begin_time']=testdata['nginxtime']-testdata['begin_time'] ##请求会话时间 与 请求到达服务时间的差\n",
    "\n",
    "##读入训练数据：\n",
    "traindata= pd.read_csv(\"round1_iflyad_anticheat_traindata.txt\",sep='\\t')\n",
    "\n",
    "traindata['begin_time']=traindata['sid'].apply(lambda x:int(x.split('-')[-1]))\n",
    "traindata['nginxtime-begin_time']=traindata['nginxtime']-traindata['begin_time']\n",
    "\n",
    "##结合数据，方便提取特征：axis=0 纵向合并；axis=1 横向合并\n",
    "data=pd.concat([traindata,testdata],axis=0).reset_index(drop=True)\n",
    "\n",
    "print('the shape of data:',data.shpe)\n",
    "\n",
    "print(data.nunique()) ##返回每个字段的所有值组成集合的大小，即集合元素个数\n",
    "print(data[:5]) ##输出数据前5行\n",
    "z=calculate_null(testdata,'sid','ver') ##计算缺失值的，下面还没用到\n",
    "\n",
    "print('label distribution:\\n',traindata['label'].value_counts()) ##查看训练集标签分布\n",
    "\n",
    "object_cols=list(data.dtypes[data.dtypes==np.object].index) ##返回字段名为object类型的字段\n",
    "print(data.dtypes[data.dtypes==np.object].index) ##输出object类型的字段\n",
    "\n",
    "##本题所给时间戳为毫秒级，故需除以1000转换为秒级：时间戳转成日期格式\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime(data['nginxtime'][0]/1000)))\n",
    "\n",
    "##对object类型的字段进行标签编码：\n",
    "for col in object_cols:\n",
    "    if col!='sid':\n",
    "        data[col]=one_hot_col(data[col].astype(str)).transform(data[col].astype(str))\n",
    "\n",
    "##划分数据：\n",
    "train=data[:traindata.shape[0]]\n",
    "label=train['label'].values\n",
    "test=data[traindata.shape[0]:].reset_index(drop=True)\n",
    "\n",
    "##模型训练预测：\n",
    "oof_lgb,prediction_lgb,feature_importance_df=\\\n",
    "      lgb_model(np.array(train.drop(['sid','label','nginxtime','ip','reqrealip','begin_time'],axis=1)),\\\n",
    "                label,\\\n",
    "                np.array(test.drop(['sid','label','nginxtime','ip','reqrealip','begin_time'],axis=1)))\n",
    "\n",
    "##保存结果：\n",
    "sub=test[['sid']]\n",
    "sub['label']=prediction_lgb\n",
    "sub['label']=sub['label'].apply(lambda x: 1 if x>0.5 else 0) ##∪概率大于0.5的置1，否则置0\n",
    "print('test pre_label distribution:\\n',sub['label'].value_counts()) ## 模型预测测试集的标签分布\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
